{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaewonHwang02/Akcse_project2024/blob/Model-Insertion/AKCSE_Summer_Project_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADDKzdzRVtwk"
      },
      "source": [
        "#ED Trigger Detection Model\n",
        "The objective of this training is to obtain the most accurate ML model capable of detecting ED (Specific or not) triggers from titles and transcripts of Youtube Videos.\n",
        "\n",
        "The experiment consists of manipulating vrious preprocessing methods, finetuning parameters and of exploring different types of models, which will result in the most accurate model for use.\n",
        "\n",
        "The models that we selected are Naive Bayes and Logistic Regression (for now).\n",
        "\n",
        "The dataset was scraped from various Youtube videos, the resulting text transformed to a csv file with columns for title, transcript and \"trigger level\". This trigger level is a binary classifier that determines if a text is capable of triggering the ED in question (i.e., 0 for \"no trigger\" and 1 for \"trigger\").\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CCXg4R80v2ll"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/taewonhwang/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtS7Uk7Xv6GJ",
        "outputId": "7c361f70-acbe-446c-bdce-f6f875e42cac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/taewonhwang/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/taewonhwang/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/taewonhwang/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9GkHKaiotTX"
      },
      "source": [
        "# Transforming Webscraping Output to Desired Format\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Although rather irrelevant to the actual code that trains the model, we will be applying our model in a Chrome extension that scrapes YouTube video titles and transcript.\n",
        "\n",
        "This information provides us with the type of data we will be dealing with as input to the created model, thus allowing us to train our binary classifier more relevant to its actual use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vkgqznwo0fl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_4UGEQ0o7vF"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We will be training our model with 200 lines of titles and transcript, obtained from webscraping on YouTube.\n",
        "All lines have been extracted from our webscraping code, representing its actual use inside of the Chrome extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFXYP0tBRLeI",
        "outputId": "a4a8f40b-d071-4974-b88b-9f4f7a8333ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file 'all_content.csv' created successfully!\n",
            "                                             content  trigger level\n",
            "0                  WIEIAD Obese *Unhealthy* Part 4\\n              1\n",
            "1  [Music] what I eat in a day what I eat in a da...              1\n",
            "2  WHAT I EAT IN A DAY AS A FAT PERSON - CALORIE ...              1\n",
            "3  [Music] what e a fat [\\xa0__\\xa0] who very con...              1\n",
            "4             TikToks That Will Make You Jealous 2\\n              1\n"
          ]
        }
      ],
      "source": [
        "# Read the content of \"Triggers.txt\"\n",
        "with open(\"Triggers.txt\", \"r\", encoding=\"utf-8\") as trigger_file:\n",
        "    trigger_content = trigger_file.readlines()\n",
        "\n",
        "# Read the content of \"Normal.txt\"\n",
        "with open(\"Normal.txt\", \"r\", encoding=\"utf-8\") as notrigger_file:\n",
        "    notrigger_content = notrigger_file.readlines()\n",
        "\n",
        "# Create DataFrames for trigger and non-trigger data\n",
        "trigger_df = pd.DataFrame({'content': trigger_content, 'trigger level': 1})\n",
        "notrigger_df = pd.DataFrame({'content': notrigger_content, 'trigger level': 0})\n",
        "\n",
        "# Concatenate the DataFrames\n",
        "df = pd.concat([trigger_df, notrigger_df], ignore_index=True)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('all_content.csv', index=False)\n",
        "\n",
        "print(\"CSV file 'all_content.csv' created successfully!\")\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsksjV6Oo7IP"
      },
      "source": [
        "#Preprocessing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q_VaEbzRf3I"
      },
      "source": [
        "Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bY6nb7kTwBet"
      },
      "outputs": [],
      "source": [
        "def lemmatize(content):\n",
        "  # Initialize lemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  # Tokenize text (useful for preprocessing)\n",
        "  con = content.split()\n",
        "  #lemmatize text\n",
        "  con_lemma = [lemmatizer.lemmatize(token) for token in con]\n",
        "  con_lemma = ' '.join(con)\n",
        "  return con_lemma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTeUyqJBRnNp"
      },
      "source": [
        "Word stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BtCxmWbJwBQ6"
      },
      "outputs": [],
      "source": [
        "def wordStem(content):\n",
        "  # Initialize Stemmer\n",
        "  stemmer = PorterStemmer()\n",
        "  # Tokenize text (useful for preprocessing)\n",
        "  con = content.split()\n",
        "  # Use the Stemmer\n",
        "  con_stem = [stemmer.stem(token) for token in con]\n",
        "  con_stem = ' '.join(con)\n",
        "  return con_stem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx0Evh6JRpN5"
      },
      "source": [
        "Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WlIeATInQDz6"
      },
      "outputs": [],
      "source": [
        "def removeStopWord(content):\n",
        "  # get english stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # Tokenize text (useful for preprocessing)\n",
        "  con = content.split()\n",
        "  # Remove stopwords\n",
        "  con_stop = [token for token in con if token.lower() not in stop_words]\n",
        "\n",
        "  con_stop = ' '.join(con)\n",
        "  return con_stop.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7aM_kpIZ9rr"
      },
      "source": [
        "# Training the Model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "For all models, we will manipulate the above preprocessing methods,and finetune the parameters to get the most accurate model for our application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDk-CZ7kpyOL"
      },
      "source": [
        "## Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8eIZEZ-a7cT"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e_roaHjInxJy"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/Users/taewonhwang/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/share/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/Users/taewonhwang/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/share/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrigger level\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# lemmatization\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m x_lemma \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Word Stemming\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m      5\u001b[0m con \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#lemmatize text\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m con_lemma \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m con]\n\u001b[1;32m      8\u001b[0m con_lemma \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(con)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m con_lemma\n",
            "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m con \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#lemmatize text\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m con_lemma \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m con]\n\u001b[1;32m      8\u001b[0m con_lemma \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(con)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m con_lemma\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/Users/taewonhwang/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/share/nltk_data'\n    - '/Users/taewonhwang/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# No preprocessing\n",
        "x = df['content']\n",
        "y = df['trigger level']\n",
        "\n",
        "# lemmatization\n",
        "df['lemma'] = df['content'].apply(lemmatize)\n",
        "x_lemma = df['lemma']\n",
        "\n",
        "# Word Stemming\n",
        "df['word stem'] = df['content'].apply(wordStem)\n",
        "x_stem = df['word stem']\n",
        "\n",
        "# Stop Word Removal\n",
        "df['stop word'] = df['content'].apply(removeStopWord)\n",
        "x_stopword = df['stop word']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9tL6GnxCm9r",
        "outputId": "3a9f4b10-a245-4e61-a4b2-a4073c4e1024"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGfuMdmntvGN"
      },
      "source": [
        "Split dataset into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h9tC4HQWD4R"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "vect = TfidfVectorizer()\n",
        "\n",
        "# No preprocessing\n",
        "x_train , x_test , y_train, y_test = train_test_split(x, y, test_size=0.20)\n",
        "\n",
        "x_train=vect.fit_transform(x_train)\n",
        "x_test=vect.transform(x_test)\n",
        "\n",
        "# Lemmatization\n",
        "x_lemma_train , x_lemma_test , y_train, y_test = train_test_split(x_lemma, y, test_size=0.20)\n",
        "\n",
        "x_lemma_train=vect.fit_transform(x_lemma_train)\n",
        "x_lemma_test=vect.transform(x_lemma_test)\n",
        "\n",
        "# Word Stemming\n",
        "x_stem_train , x_stem_test , y_train, y_test = train_test_split(x_stem, y, test_size=0.20)\n",
        "\n",
        "x_stem_train=vect.fit_transform(x_stem_train)\n",
        "x_stem_test=vect.transform(x_stem_test)\n",
        "\n",
        "# Stop Word Removal\n",
        "x_stopword_train , x_stopword_test , y_train, y_test = train_test_split(x_stopword, y, test_size=0.20)\n",
        "\n",
        "x_stopword_train=vect.fit_transform(x_stopword_train)\n",
        "x_stopword_test=vect.transform(x_stopword_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ3ZL6gIqoAY"
      },
      "source": [
        "##Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ2D9cyR8xN7",
        "outputId": "869e55cc-c78d-4cbc-e316-4e224d96420b"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "# No Preprocessing\n",
        "# Instance of NB\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(x_train.toarray(), y_train)\n",
        "# Make predictions\n",
        "nb_predictions = nb_model.predict(x_test.toarray())\n",
        "# Evaluate the model\n",
        "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
        "nb_report = classification_report(y_test, nb_predictions)\n",
        "nb_confusion = confusion_matrix(y_test, nb_predictions)\n",
        "\n",
        "print(\"Naive Bayes No Preprocessing Accuracy:\", nb_accuracy)\n",
        "print(\"Naive Bayes No Preprocessing Report:\", nb_report)\n",
        "\n",
        "\n",
        "# Lemmatization\n",
        "# Instance of NB\n",
        "nb_lemma_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_lemma_model.fit(x_lemma_train.toarray(), y_train)\n",
        "# Make predictions\n",
        "nb_lemma_predictions = nb_lemma_model.predict(x_lemma_test.toarray())\n",
        "# Evaluate the model\n",
        "nb_lemma_accuracy = accuracy_score(y_test, nb_lemma_predictions)\n",
        "\n",
        "print(\"Naive Bayes Lemmatization Accuracy:\", nb_lemma_accuracy)\n",
        "\n",
        "\n",
        "# Word Stemming\n",
        "# Instance of NB\n",
        "nb_stem_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_stem_model.fit(x_stem_train.toarray(), y_train)\n",
        "# Make predictions\n",
        "nb_stem_predictions = nb_stem_model.predict(x_stem_test.toarray())\n",
        "# Evaluate the model\n",
        "nb_stem_accuracy = accuracy_score(y_test, nb_stem_predictions)\n",
        "\n",
        "print(\"Naive Bayes Word Stem Accuracy:\", nb_stem_accuracy)\n",
        "\n",
        "\n",
        "# Stop Word Removal\n",
        "# Instance of NB\n",
        "nb_stopword_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_stopword_model.fit(x_stopword_train.toarray(), y_train)\n",
        "# Make predictions\n",
        "nb_stopword_predictions = nb_stopword_model.predict(x_stopword_test.toarray())\n",
        "# Evaluate the model\n",
        "nb_stopword_accuracy = accuracy_score(y_test, nb_stopword_predictions)\n",
        "\n",
        "print(\"Naive Bayes Stop Word Removal Accuracy:\", nb_stopword_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKVqnNf_soZs"
      },
      "source": [
        "### Finetuning\n",
        "For a Gaussian Naive Bayes model, we will be using a grid search to explore the effects of smoothing variables on the overall performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBqdq7SGsqd5",
        "outputId": "c411e90c-6eb2-4c64-8dec-4e674b81c7d8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "# No preprocessing\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(nb_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(x_train.toarray(), y_train)\n",
        "\n",
        "# Get the best model from grid search\n",
        "finetune_best = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "predictions = finetune_best.predict(x_test.toarray())\n",
        "\n",
        "# Evaluate the model\n",
        "finetune_accuracy = accuracy_score(y_test, predictions)\n",
        "finetune_report = classification_report(y_test, predictions)\n",
        "\n",
        "# Display results\n",
        "print(\"Gaussian Naive Bayes Model Accuracy:\", finetune_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", finetune_report)\n",
        "\n",
        "\n",
        "# Lemmatization\n",
        "grid_search = GridSearchCV(nb_lemma_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(x_lemma_train.toarray(), y_train)\n",
        "\n",
        "finetune_lemma_best = grid_search.best_estimator_\n",
        "predictions_lemma = finetune_lemma_best.predict(x_lemma_test.toarray())\n",
        "\n",
        "# Evaluate the model\n",
        "finetune_lemma_accuracy = accuracy_score(y_test, predictions_lemma)\n",
        "finetune_lemma_report = classification_report(y_test, predictions_lemma)\n",
        "\n",
        "# Display results\n",
        "print(\"Lemma Accuracy:\", finetune_lemma_accuracy)\n",
        "print(\"Lemma Classification Report:\", finetune_lemma_report)\n",
        "\n",
        "\n",
        "\n",
        "# Word Stemming\n",
        "grid_search = GridSearchCV(nb_stem_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(x_stem_train.toarray(), y_train)\n",
        "\n",
        "finetune_stem_best = grid_search.best_estimator_\n",
        "predictions_stem = finetune_stem_best.predict(x_stem_test.toarray())\n",
        "\n",
        "# Evaluate the model\n",
        "finetune_stem_accuracy = accuracy_score(y_test, predictions_stem)\n",
        "finetune_stem_report = classification_report(y_test, predictions_stem)\n",
        "\n",
        "# Display results\n",
        "print(\"Word Stem Accuracy:\", finetune_stem_accuracy)\n",
        "print(\"Word Stem Classification Report:\", finetune_stem_report)\n",
        "\n",
        "\n",
        "\n",
        "# Stop Word Removal\n",
        "grid_search = GridSearchCV(nb_stopword_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(x_stopword_train.toarray(), y_train)\n",
        "\n",
        "finetune_stopword_best = grid_search.best_estimator_\n",
        "predictions_stopword = finetune_stopword_best.predict(x_stopword_test.toarray())\n",
        "\n",
        "# Evaluate the model\n",
        "finetune_stopword_accuracy = accuracy_score(y_test, predictions_stopword)\n",
        "finetune_stopword_report = classification_report(y_test, predictions_stopword)\n",
        "\n",
        "# Display results\n",
        "print(\"Stop Word Accuracy:\", finetune_stopword_accuracy)\n",
        "print(\"Stop Word Classification Report:\", finetune_stopword_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpi1TMrolfi3"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "My_x0bA_lfLb",
        "outputId": "17dbabb3-7996-4a3b-df9b-76bb85545a8c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "Categories = ['No Preprocessing', 'Lemmatization', 'Word Stemming', 'Stop Word Removal']\n",
        "no_finetune = [nb_accuracy, nb_lemma_accuracy, nb_stem_accuracy, nb_stopword_accuracy]\n",
        "finetuning = [finetune_accuracy, finetune_lemma_accuracy, finetune_stem_accuracy, finetune_stopword_accuracy]\n",
        "\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(Categories))\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.bar(index, no_finetune, bar_width, label='No Finetuning', color='skyblue')\n",
        "plt.bar(index + bar_width, finetuning, bar_width, label='Finetuning')\n",
        "\n",
        "plt.xlabel('Preprocessing Methods')\n",
        "plt.ylabel('Model Performance Accuracy')\n",
        "plt.title('Naive Bayes Accuracy Comparison (Preprocessing & Finetuning)')\n",
        "plt.xticks(index + bar_width / 2, Categories)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJqRYIx8o6zI"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ6iv34t8xyj",
        "outputId": "ba87abd8-3b03-489e-eb5e-8b1674d90e3e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "# No Preprocessing\n",
        "# Create a Logistic Regression model\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(x_train, y_train)\n",
        "# Make predictions\n",
        "lr_predictions = lr_model.predict(x_test)\n",
        "# Evaluate the model\n",
        "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
        "\n",
        "# Lemmatization\n",
        "# Create a Logistic Regression model\n",
        "lr_lemma_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_lemma_model.fit(x_lemma_train, y_train)\n",
        "# Make predictions\n",
        "lr_lemma_predictions = lr_lemma_model.predict(x_lemma_test)\n",
        "# Evaluate the model\n",
        "lr_lemma_accuracy = accuracy_score(y_test, lr_lemma_predictions)\n",
        "\n",
        "print(\"Logistic Regression Lemmatization Accuracy:\", lr_lemma_accuracy)\n",
        "\n",
        "# Word Stemming\n",
        "# Create a Logistic Regression model\n",
        "lr_stem_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_stem_model.fit(x_stem_train, y_train)\n",
        "# Make predictions\n",
        "lr_stem_predictions = lr_stem_model.predict(x_stem_test)\n",
        "# Evaluate the model\n",
        "lr_stem_accuracy = accuracy_score(y_test, lr_stem_predictions)\n",
        "\n",
        "print(\"Logistic Regression Word Stemming Accuracy:\", lr_stem_accuracy)\n",
        "\n",
        "# Stop Word Removal\n",
        "# Create a Logistic Regression model\n",
        "lr_stopword_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_stopword_model.fit(x_stopword_train, y_train)\n",
        "# Make predictions\n",
        "lr_stopword_predictions = lr_stopword_model.predict(x_stopword_test)\n",
        "# Evaluate the model\n",
        "lr_stopword_accuracy = accuracy_score(y_test, lr_stopword_predictions)\n",
        "\n",
        "print(\"Logistic Regression Stop Word Accuracy:\", lr_stopword_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Y5iFyAsrgX"
      },
      "source": [
        "### Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2dET8Gass8o",
        "outputId": "9d11cbea-1116-4c8b-8e2f-fbb63b9434e5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters and their respective ranges to tune\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'max_iter': [100, 200, 500]\n",
        "}\n",
        "\n",
        "# No preprocessing\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(lr_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(x_train, y_train)\n",
        "# Get the best model\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "lr_predictions = best_lr_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "best_lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
        "lr_report = classification_report(y_test, lr_predictions, zero_division=0)\n",
        "\n",
        "# Display results\n",
        "print(\"Raw Logistic Regression Model Accuracy:\", best_lr_accuracy)\n",
        "print(\"Classification Report:\", lr_report)\n",
        "\n",
        "\n",
        "# Lemmatization\n",
        "grid_search = GridSearchCV(lr_lemma_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(x_lemma_train, y_train)\n",
        "best_lr_model_lemma = grid_search.best_estimator_\n",
        "lr_lemma_predictions = best_lr_model_lemma.predict(x_lemma_test)\n",
        "\n",
        "# Evaluate the model\n",
        "best_lr_lemma_accuracy = accuracy_score(y_test, lr_lemma_predictions)\n",
        "lr_lemma_report = classification_report(y_test, lr_lemma_predictions, zero_division=0)\n",
        "\n",
        "print(\"Lemmatized Logistic Regression Model Accuracy:\", best_lr_lemma_accuracy)\n",
        "print(\"Classification Report:\", lr_lemma_report)\n",
        "\n",
        "\n",
        "# Word Stemming\n",
        "grid_search = GridSearchCV(lr_stem_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(x_stem_train, y_train)\n",
        "best_lr_model_stem = grid_search.best_estimator_\n",
        "lr_stem_predictions = best_lr_model_stem.predict(x_stem_test)\n",
        "\n",
        "# Evaluate the model\n",
        "best_lr_stem_accuracy = accuracy_score(y_test, lr_stem_predictions)\n",
        "lr_stem_report = classification_report(y_test, lr_stem_predictions, zero_division=0)\n",
        "\n",
        "print(\"Stemmed Logistic Regression Model Accuracy:\", best_lr_stem_accuracy)\n",
        "print(\"Classification Report:\", lr_stem_report)\n",
        "\n",
        "\n",
        "\n",
        "# Stop Word Removal\n",
        "grid_search = GridSearchCV(lr_stopword_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(x_stopword_train, y_train)\n",
        "best_lr_model_stopword = grid_search.best_estimator_\n",
        "lr_stopword_predictions = best_lr_model_stopword.predict(x_stopword_test)\n",
        "\n",
        "# Evaluate the model\n",
        "best_lr_stopword_accuracy = accuracy_score(y_test, lr_stopword_predictions)\n",
        "lr_stopword_report = classification_report(y_test, lr_stopword_predictions, zero_division=0)\n",
        "\n",
        "print(\"Stop Word Removed Logistic Regression Model Accuracy:\", best_lr_stopword_accuracy)\n",
        "print(\"Classification Report:\", lr_stopword_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YJK3UBSpTy7"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "x139zgSFjCSZ",
        "outputId": "ce87d2df-99b1-4d43-eb56-a8178f1d47f9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "Categories = ['No Preprocessing', 'Lemmatization', 'Word Stemming', 'Stop Word Removal']\n",
        "no_finetune = [lr_accuracy, lr_lemma_accuracy, lr_stem_accuracy, lr_stopword_accuracy]\n",
        "finetuning = [best_lr_accuracy, best_lr_lemma_accuracy, best_lr_stem_accuracy, best_lr_stopword_accuracy]\n",
        "\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(Categories))\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.bar(index, no_finetune, bar_width, label='No Finetuning', color='limegreen')\n",
        "plt.bar(index + bar_width, finetuning, bar_width, label='Finetuning', color='g')\n",
        "\n",
        "plt.xlabel('Preprocessing Methods')\n",
        "plt.ylabel('Model Performance Accuracy')\n",
        "plt.title('Logistic Regression Accuracy Comparison (Preprocessing & Finetuning)')\n",
        "plt.xticks(index + bar_width / 2, Categories)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAx8A6V3vEu6"
      },
      "source": [
        "#Model Selection\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Once all models and their respective manipulations have been performed and evaluated, now it is time to select and implement the final model that will be used for the chrome extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apxtcCjlZWJP"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "x_train = x_stopword_train\n",
        "x_test = x_stopword_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B2c8U0oT2Au"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "# Save the final content and model\n",
        "pickle.dump(vect, open('vector.pkl', 'wb'))\n",
        "pickle.dump(finetune_stopword_best, open('model.pkl', 'wb'))\n",
        "\n",
        "# Load the saved model\n",
        "vector_model = pickle.load(open('vector.pkl', 'rb'))\n",
        "model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O7G6m7mUW7U"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "def EDTrigger(content):\n",
        "  # preprocessing of choice\n",
        "  content = removeStopWord(content)\n",
        "\n",
        "  # convert text to vector\n",
        "  input = [content]\n",
        "  vector = vector.transform(input)\n",
        "\n",
        "  # predict using final model\n",
        "  prediction = model.predict(vector)\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
